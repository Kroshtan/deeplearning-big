{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep_learning2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vfk-1xcUQ3Vh","colab_type":"code","outputId":"22a726d6-34a9-49c8-8418-8d7f4421c8b4","executionInfo":{"status":"ok","timestamp":1560348307700,"user_tz":-120,"elapsed":6250,"user":{"displayName":"kupke2011@live.nl","photoUrl":"","userId":"05667716658323280448"}},"colab":{"base_uri":"https://localhost:8080/","height":330}},"source":["#Package needed for next code block\n","!pip3 install PyDrive"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting PyDrive\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n","\u001b[K     |████████████████████████████████| 993kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n","Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n","Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n","Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n","Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.5)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.5)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n","Building wheels for collected packages: PyDrive\n","  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n","Successfully built PyDrive\n","Installing collected packages: PyDrive\n","Successfully installed PyDrive-1.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wFuX3W13RDsG","colab_type":"code","colab":{}},"source":["#Code to authorize Colab to access Google Drive\n","import os\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gljUxy3jRFnD","colab_type":"code","colab":{}},"source":["#Code for reading the data from Google Drive\n","#Non-augmented train data\n","download = drive.CreateFile({'id': '1-3R9ZN-yRXLmsm4oGmmuHM_REUwHEen3'})\n","download.GetContentFile('traindata.npy')\n","\n","#Augmented train data\n","download = drive.CreateFile({'id': '1aVjdTfLfkpL5EEAO3MQgXh69XqATCzvI'})\n","download.GetContentFile('robin_data_0.npy')\n","\n","download = drive.CreateFile({'id': '1xL0x8M9zmHkjvbAc8gf8R8JKYl_t_Gdu'})\n","download.GetContentFile('complex_data_0.npy')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dSDOip4GRHSG","colab_type":"code","cellView":"both","outputId":"1686610d-9d09-4d8c-c016-7b62bcde17d7","executionInfo":{"status":"error","timestamp":1560352376383,"user_tz":-120,"elapsed":192819,"user":{"displayName":"kupke2011@live.nl","photoUrl":"","userId":"05667716658323280448"}},"colab":{"base_uri":"https://localhost:8080/","height":2069}},"source":["#@title  { form-width: \"250px\" }\n","from __future__ import print_function, division\n","from keras.datasets import mnist\n","from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Conv2D\n","from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n","from keras.layers import Conv2DTranspose, MaxPooling2D, Concatenate, LeakyReLU\n","from keras.layers import Dropout\n","from keras.layers.convolutional import UpSampling2D, Conv2D\n","from keras.models import Sequential, Model, load_model\n","from keras.optimizers import Adam\n","from keras.preprocessing.image import load_img, img_to_array, array_to_img\n","from keras.callbacks import TensorBoard\n","from PIL import Image\n","import PIL.ImageOps\n","import matplotlib.pyplot as plt\n","import os\n","import shutil\n","import glob, os\n","from PIL import Image\n","import PIL.ImageOps\n","import matplotlib.pyplot as plt\n","import sys\n","import numpy as np\n","\n","PREPARE_COLAB_DATA = False\n","RUN_ON_COLAB = True\n","NPY_SAVEFILE = 'traindata.npy'\n","IMAGE_DIR = 'images/'\n","TRAIN_ON_AUGMENTED = True\n","\n","SIMPLE_DATA = ['./robin_data_0.npy']\n","COMPLEX_DATA = ['./complex_data_0.npy']\n","\n","EPOCHS = 30000\n","BATCH_SIZE = 16\n","SAMPLE_INTERVAL = 100\n","RESCALE_FACTOR = 32\n","TRAIN_ON_COMPLEX = False\n","\n","class GAN():\n","    def __init__(self):\n","        self.channels = 1\n","        self.latent_dim = 300\n","\n","        optimizer = Adam(1e-3, decay=1e-4)\n","\n","        self.logdir = \"./logs\"\n","\n","        if not RUN_ON_COLAB:\n","            # Empty any old log directory\n","            if os.path.exists(self.logdir):\n","                shutil.rmtree(self.logdir)\n","                print(\"Removed old log directory.\")\n","\n","            os.mkdir(self.logdir)\n","            print('Created new log directory.')\n","\n","\n","        try:\n","            # Empty the generated image directory\n","            for the_file in os.listdir(\"./images\"):\n","                file_path = os.path.join(\"./images\", the_file)\n","                try:\n","                    if os.path.isfile(file_path):\n","                        os.unlink(file_path)\n","                    elif os.path.isdir(file_path): shutil.rmtree(file_path)\n","                except Exception as e:\n","                    print(e)\n","        except:\n","            print(\"./images dir does not yet exist\")\n","\n","\n","        # Load the dataset\n","        filelist = glob.glob(\"./source_imgs/*.jpg\")\n","        imgs = [Image.open(fname) for fname in filelist]\n","        if RUN_ON_COLAB:\n","            try:\n","                os.mkdir(IMAGE_DIR)\n","                print(\"Created output images directory...\")\n","            except:\n","                print(\"Output images directory already exists!\")\n","            if TRAIN_ON_AUGMENTED:\n","                self.X_train = np.load(SIMPLE_DATA[0], allow_pickle=True)\n","            else:\n","                self.X_train = np.stack(np.load(NPY_SAVEFILE, allow_pickle=True))\n","            self.X_train = np.expand_dims(self.X_train, axis=3)\n","            target_size = (max([x.shape[1] for x in self.X_train]), max([x.shape[0] for x in self.X_train]))\n","            #target_size = (self.X_train[0].shape[1], self.X_train[1].shape[0])\n","            self.img_shape = (target_size[1], target_size[0], self.channels)\n","\n","        if PREPARE_COLAB_DATA:\n","            np.save(NPY_SAVEFILE, self.X_train)\n","            #quit()\n","\n","        # Build and compile the discriminator\n","        self.discriminator = self.build_discriminator()\n","        self.discriminator.compile(loss='binary_crossentropy',\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","\n","        # Build the generator\n","        self.generator = self.build_generator()\n","\n","        # The generator takes noise as input and generates imgs\n","        z = Input(shape=(self.latent_dim,))\n","        img = self.generator(z)\n","\n","        # For the combined model we will only train the generator\n","        self.discriminator.trainable = False\n","\n","        # The discriminator takes generated images as input and determines validity\n","        validity = self.discriminator(img)\n","\n","        # The combined model  (stacked generator and discriminator)\n","        # Trains the generator to fool the discriminator\n","        self.combined = Model(z, validity)\n","        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n","\n","\n","    def build_generator(self):\n","\n","        inp = Input(shape=(self.latent_dim,))\n","\n","        layer1 = Dense(128,\n","                       input_shape=(self.latent_dim,))(inp)\n","        layer1 = LeakyReLU()(layer1)\n","        layer1 = BatchNormalization(momentum=0.8)(layer1)\n","        layer1 = Dropout(rate=0.1)(layer1)\n","\n","        layer2 = Dense(512)(layer1)\n","        layer2 = LeakyReLU()(layer2)\n","        layer2 = BatchNormalization(momentum=0.8)(layer2)\n","        layer2 = Dropout(rate=0.1)(layer2)\n","\n","        layer3 = Dense(256)(layer2)\n","        layer3 = LeakyReLU()(layer3)\n","        layer3 = BatchNormalization(momentum=0.8)(layer3)\n","        layer3 = Dropout(rate=0.1)(layer3)\n","\n","        # layer4 = Dense(16)(layer3)\n","        # layer4 = LeakyReLU()(layer4)\n","        # layer4 = BatchNormalization(momentum=0.8)(layer4)\n","\n","        concat = Concatenate(axis=-1)([layer1, layer2, layer3])\n","\n","        pre_out = Dense(np.prod(self.img_shape), activation='tanh')(concat)\n","\n","        out = Reshape(target_shape=(self.img_shape))(pre_out)\n","\n","        model = Model(inputs=inp, outputs=out)\n","\n","        model.summary()\n","\n","        return model\n","\n","    def build_discriminator(self):\n","\n","        inp = Input(shape=self.img_shape)\n","\n","        conv1 = Conv2D(filters=24,\n","                       kernel_size=(5, 5),\n","                       activation='relu',\n","                       padding='same')(inp)\n","        conv1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","        flat_conv1 = Flatten()(conv1)\n","        flat_conv1 = Dense(512, activation='relu')(flat_conv1)\n","\n","        conv2 = Conv2D(filters=24,\n","                       kernel_size=(5, 5),\n","                       activation='relu',\n","                       padding='same')(conv1)\n","        conv2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","        flat_conv2 = Flatten()(conv2)\n","        flat_conv2 = Dense(512, activation='relu')(flat_conv2)\n","\n","\n","        conv3 = Conv2D(filters=16,\n","                       kernel_size=(5, 5),\n","                       activation='relu',\n","                       padding='same')(conv2)\n","        conv3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","        flat_conv3 = Flatten()(conv3)\n","        flat_conv3 = Dense(512, activation='relu')(flat_conv3)\n","\n","\n","        fc = Concatenate()([flat_conv1, flat_conv2, flat_conv3])\n","\n","        fc = Dense(1024, activation='relu')(fc)\n","\n","        out = Dense(1, activation='sigmoid')(fc)\n","\n","        model = Model(inputs=inp, outputs=out)\n","        model.summary()\n","\n","        return model\n","\n","    def train(self, epochs, batch_size=1, sample_interval=50):\n","\n","        # Adversarial ground truths\n","        valid = np.ones((batch_size, 1))\n","        fake = np.zeros((batch_size, 1))\n","\n","        tensorboard = TensorBoard(log_dir=self.logdir)\n","        tensorboard.set_model(self.discriminator)\n","        for epoch in range(epochs):\n","\n","            # ---------------------\n","            #  Train Discriminator\n","            # ---------------------\n","\n","            # Select a random batch of images\n","            idx = np.random.randint(0, len(self.X_train), batch_size)\n","            imgs = self.X_train[idx]\n","\n","            noise = np.random.normal(-1, 1, (batch_size, self.latent_dim))\n","\n","            # Generate a batch of new images\n","            gen_imgs = self.generator.predict(noise)\n","\n","            if epoch == 0 or accuracy < 80:\n","                # Train the discriminator\n","                d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n","                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n","                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","            else:\n","                # Test the discriminator\n","                d_loss_real = self.discriminator.test_on_batch(imgs, valid)\n","                d_loss_fake = self.discriminator.test_on_batch(gen_imgs, fake)\n","                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","            accuracy = 100*d_loss[1]\n","\n","            # ---------------------\n","            #  Train Generator\n","            # ---------------------\n","            noise = np.random.normal(-1, 1, (batch_size, self.latent_dim))\n","\n","            if epoch == 0 or accuracy > 20:\n","                # Train the generator (to have the discriminator label samples as valid)\n","                g_loss = self.combined.train_on_batch(noise, valid)\n","            else:\n","                # Train the generator (to have the discriminator label samples as valid)\n","                g_loss = self.combined.test_on_batch(noise, valid)\n","\n","            tensorboard.on_epoch_end(epoch, {'generator loss': g_loss, 'discriminator loss': d_loss[0], 'Accuracy': accuracy})\n","\n","            # Plot the progress\n","            if RUN_ON_COLAB:\n","                if (epoch % 50) == 0:\n","                    print(f\"{epoch} [D loss: {d_loss[0]}, \" +\n","                  f\"acc.: {accuracy}%] [G loss: {g_loss}]\")\n","            else:\n","                print(f\"{epoch} [D loss: {d_loss[0]:.3f}, \" +\n","                  f\"acc.: {accuracy:.2f}%] [G loss: {g_loss:.3f}]\")\n","\n","            # If at save interval => save generated image samples\n","            if epoch % sample_interval == 0:\n","                self.sample_images(epoch)\n","        tensorboard.on_train_end()\n","        self.discriminator.save('discriminator.h5')\n","        self.generator.save('generator.h5')\n","\n","    def sample_images(self, epoch):\n","        r, c = 3, 3\n","        noise = np.random.normal(-1, 1, (r * c, self.latent_dim))\n","        gen_imgs = self.generator.predict(noise)\n","\n","        # Rescale images from [-1, 1] to [1, 0] (invert)\n","        real_imgs = self.X_train[np.random.choice(self.X_train.shape[0], size=c), :, :, 0]\n","        gen_imgs = -0.5 * gen_imgs - 0.5\n","        real_imgs = -0.5 * real_imgs - 0.5\n","\n","        fig, axs = plt.subplots(1+r, c)\n","\n","        for j in range(c):\n","            axs[0,j].imshow(real_imgs[j], cmap='gray')\n","            axs[0,j].axis('off')\n","\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i+1,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n","                axs[i+1,j].axis('off')\n","                cnt += 1\n","\n","        fig.savefig(IMAGE_DIR+\"%d.png\" % epoch)\n","        plt.close()\n","\n","\n","if __name__ == '__main__':\n","    gan = GAN()\n","    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","Output images directory already exists!\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 230, 230, 1)  0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 230, 230, 24) 624         input_1[0][0]                    \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 115, 115, 24) 0           conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 115, 115, 24) 14424       max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 57, 57, 24)   0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 57, 57, 16)   9616        max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","max_pooling2d_3 (MaxPooling2D)  (None, 28, 28, 16)   0           conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 317400)       0           max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","flatten_2 (Flatten)             (None, 77976)        0           max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","flatten_3 (Flatten)             (None, 12544)        0           max_pooling2d_3[0][0]            \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 512)          162509312   flatten_1[0][0]                  \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 512)          39924224    flatten_2[0][0]                  \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 512)          6423040     flatten_3[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 1536)         0           dense_1[0][0]                    \n","                                                                 dense_2[0][0]                    \n","                                                                 dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 1024)         1573888     concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 1)            1025        dense_4[0][0]                    \n","==================================================================================================\n","Total params: 210,456,153\n","Trainable params: 210,456,153\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            (None, 300)          0                                            \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 128)          38528       input_2[0][0]                    \n","__________________________________________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)       (None, 128)          0           dense_6[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 128)          0           leaky_re_lu_1[0][0]              \n","__________________________________________________________________________________________________\n","dense_7 (Dense)                 (None, 512)          66048       dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)       (None, 512)          0           dense_7[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 512)          0           leaky_re_lu_2[0][0]              \n","__________________________________________________________________________________________________\n","dense_8 (Dense)                 (None, 256)          131328      dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)       (None, 256)          0           dense_8[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 256)          0           leaky_re_lu_3[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 896)          0           dropout_1[0][0]                  \n","                                                                 dropout_2[0][0]                  \n","                                                                 dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","dense_9 (Dense)                 (None, 52900)        47451300    concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","reshape_1 (Reshape)             (None, 230, 230, 1)  0           dense_9[0][0]                    \n","==================================================================================================\n","Total params: 47,687,204\n","Trainable params: 47,687,204\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"},{"output_type":"stream","text":["0 [D loss: 0.36466845870018005, acc.: 50.0%] [G loss: 16.11809539794922]\n","50 [D loss: 8.05904769897461, acc.: 50.0%] [G loss: 16.11809539794922]\n","100 [D loss: 8.05904769897461, acc.: 50.0%] [G loss: 16.11809539794922]\n","150 [D loss: 8.05904769897461, acc.: 50.0%] [G loss: 16.11809539794922]\n","200 [D loss: 8.05904769897461, acc.: 50.0%] [G loss: 16.11809539794922]\n","250 [D loss: 8.05904769897461, acc.: 50.0%] [G loss: 16.11809539794922]\n","300 [D loss: 8.05904769897461, acc.: 50.0%] [G loss: 16.11809539794922]\n","350 [D loss: 8.05904769897461, acc.: 50.0%] [G loss: 16.11809539794922]\n","400 [D loss: 8.05904769897461, acc.: 50.0%] [G loss: 16.11809539794922]\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b2618ae1bed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSAMPLE_INTERVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-b2618ae1bed6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;31m# Train the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m                 \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"Tbia2zHZdFRU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}