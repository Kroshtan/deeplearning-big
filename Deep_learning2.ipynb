{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep_learning2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vfk-1xcUQ3Vh","colab_type":"code","outputId":"ab7875aa-0e92-41dc-eb7d-cf9883775926","executionInfo":{"status":"ok","timestamp":1559302991247,"user_tz":-120,"elapsed":9079,"user":{"displayName":"kupke2011@live.nl","photoUrl":"","userId":"05667716658323280448"}},"colab":{"base_uri":"https://localhost:8080/","height":330}},"source":["#Package needed for next code block\n","!pip3 install PyDrive"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting PyDrive\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n","\r\u001b[K     |▎                               | 10kB 15.6MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 4.1MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 5.8MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 3.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 4.7MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 5.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 6.4MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 7.1MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 7.9MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 6.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 6.3MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 6.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 6.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 6.3MB/s \n","\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n","Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n","Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n","Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n","Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.5)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.5)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n","Building wheels for collected packages: PyDrive\n","  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n","Successfully built PyDrive\n","Installing collected packages: PyDrive\n","Successfully installed PyDrive-1.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Bd8ockMwGQBV","colab_type":"code","colab":{}},"source":["!pip3 install tensorboardcolab"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wFuX3W13RDsG","colab_type":"code","colab":{}},"source":["#Code to authorize Colab to access Google Drive\n","import os\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gljUxy3jRFnD","colab_type":"code","colab":{}},"source":["#Code for reading the data from Google Drive\n","#Non-augmented train data\n","download = drive.CreateFile({'id': '1-3R9ZN-yRXLmsm4oGmmuHM_REUwHEen3'})\n","download.GetContentFile('traindata.npy')\n","\n","#Augmented train data\n","download = drive.CreateFile({'id': '1VOUu67nZayC63KJ6DHvBYdYbtJVwgip2'})\n","download.GetContentFile('augmented_data_0.npy')\n","download = drive.CreateFile({'id': '1gtxZ_2e_OQKXppo0CQ0VBSYKDvfN2lWN'})\n","download.GetContentFile('augmented_data_1.npy')\n","download = drive.CreateFile({'id': '1EuUwJ_2zItjKpMx9EFfEXqeUzN6YcKay'})\n","download.GetContentFile('augmented_data_2.npy')\n","download = drive.CreateFile({'id': '1DspGBsieJRYI88EGbc_ctqIMKNX7mdN5'})\n","download.GetContentFile('augmented_data_3.npy')\n","download = drive.CreateFile({'id': '1mxI_xtHzovhgAKWVFQwDxT5623ZSgDU9'})\n","download.GetContentFile('augmented_data_4.npy')\n","download = drive.CreateFile({'id': '18hPceYSPrrmnnBo9GDO5NLWjBX6Vw2r7'})\n","download.GetContentFile('augmented_data_5.npy')\n","download = drive.CreateFile({'id': '1N__M6BKzQoU-56CcePPv6tAXRb5fT6N4'})\n","download.GetContentFile('augmented_data_6.npy')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NVtH62wIG135","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":185},"outputId":"08abbaf0-d5d8-4552-94e4-62a1903ad2ec","executionInfo":{"status":"error","timestamp":1559309828764,"user_tz":-120,"elapsed":1077,"user":{"displayName":"kupke2011@live.nl","photoUrl":"","userId":"05667716658323280448"}}},"source":["import tensorboardcolab\n"],"execution_count":22,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-90330e58ff50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorboardcolab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtbc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTensorBoardColab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'TensorBoardColab' is not defined"]}]},{"cell_type":"code","metadata":{"id":"dSDOip4GRHSG","colab_type":"code","cellView":"both","outputId":"63c876f9-d0a0-4cf8-b724-2d57928a2755","executionInfo":{"status":"error","timestamp":1559312249616,"user_tz":-120,"elapsed":441739,"user":{"displayName":"kupke2011@live.nl","photoUrl":"","userId":"05667716658323280448"}},"colab":{"base_uri":"https://localhost:8080/","height":1826}},"source":["#@title  { form-width: \"250px\" }\n","from __future__ import print_function, division\n","from keras.datasets import mnist\n","from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Conv2D\n","from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import UpSampling2D, Conv2D\n","from keras.models import Sequential, Model\n","from keras.optimizers import Adam\n","from keras.preprocessing.image import load_img, img_to_array, array_to_img\n","from keras.callbacks import TensorBoard\n","from PIL import Image\n","import PIL.ImageOps\n","import matplotlib.pyplot as plt\n","import os\n","import shutil\n","import glob, os\n","from PIL import Image\n","import PIL.ImageOps\n","import matplotlib.pyplot as plt\n","import sys\n","import numpy as np\n","\n","PREPARE_COLAB_DATA = False\n","RUN_ON_COLAB = True\n","NPY_SAVEFILE = 'traindata.npy'\n","IMAGE_DIR = 'images/'\n","TRAIN_ON_AUGMENTED = False\n","AUGMENTED_FILES = ['../augmented_data_0.npy', '../augmented_data_1.npy', '../augmented_data_2.npy',\n"," '../augmented_data_3.npy', '../augmented_data_4.npy', '../augmented_data_5.npy', '../augmented_data_6.npy', ]\n","\n","EPOCHS = 30000\n","BATCH_SIZE = 16\n","SAMPLE_INTERVAL = 20\n","\n","class GAN():\n","    def __init__(self):\n","        self.channels = 1\n","        self.latent_dim = 100\n","        rescale_factor = 32\n","\n","        optimizer = Adam(0.0001, 0.5)\n","\n","        self.logdir = \"./logs\"\n","        try: \n","          os.mkdir(self.logdir)\n","          print('Created log directory...')\n","        except:\n","          print('Log directory already exists!')\n","\n","        # Empty any old log directory\n","        for the_file in os.listdir(self.logdir):\n","            file_path = os.path.join(self.logdir, the_file)\n","            try:\n","                if os.path.isfile(file_path):\n","                    os.unlink(file_path)\n","                elif os.path.isdir(file_path): shutil.rmtree(file_path)\n","            except Exception as e:\n","                print(e)\n","\n","        # Empty the generated image directory\n","        for the_file in os.listdir(\"./images\"):\n","            file_path = os.path.join(\"./images\", the_file)\n","            try:\n","                if os.path.isfile(file_path):\n","                    os.unlink(file_path)\n","                elif os.path.isdir(file_path): shutil.rmtree(file_path)\n","            except Exception as e:\n","                print(e)\n","\n","\n","        # Load the dataset\n","        filelist = glob.glob(\"./source_imgs/*.jpg\")\n","        imgs = [Image.open(fname) for fname in filelist]\n","        if RUN_ON_COLAB:\n","            try:\n","                os.mkdir(IMAGE_DIR)\n","                print(\"Created output images directory...\")\n","            except:\n","                print(\"Output images directory already exists!\")\n","            if TRAIN_ON_AUGMENTED:\n","                self.X_train = np.load(AUGMENTED_FILES[0])\n","            else:\n","                self.X_train = np.load(NPY_SAVEFILE)\n","            print(self.X_train.shape)\n","            target_size = (max([x.shape[1] for x in self.X_train]), max([x.shape[0] for x in self.X_train]))\n","            self.img_shape = (target_size[1], target_size[0], self.channels)\n","        else:\n","            # Load the dataset\n","            filelist = glob.glob(\"./source_imgs/*.jpg\")\n","            imgs = [Image.open(fname) for fname in filelist]\n","\n","            self.target_size  = (max([x.size[0] for x in imgs]),\n","                                 max([x.size[1] for x in imgs]))\n","\n","            self.target_size = tuple([x//rescale_factor for x in self.target_size])\n","\n","            self.X_train = []\n","\n","            for img in imgs:\n","                old_size = img.size\n","                ratio = min(self.target_size[0]/old_size[0],\n","                            self.target_size[1]/old_size[1])\n","\n","                new_size = tuple([int(x*ratio) for x in old_size])\n","                img = img.resize(new_size, Image.ANTIALIAS)\n","                img = PIL.ImageOps.invert(img)\n","                new_img = Image.new(\"L\", self.target_size)\n","                new_img.paste(img, ((self.target_size[0]-new_size[0])//2,\n","                                    (self.target_size[1]-new_size[1])//2))\n","                self.X_train.append(new_img)\n","\n","            self.X_train = np.stack(self.X_train)\n","\n","            self.img_shape = (self.target_size[1],\n","                              self.target_size[0],\n","                              self.channels)\n","\n","            # Rescale -1 to 1\n","            self.X_train = self.X_train / 127.5 - 1.\n","            self.X_train = np.expand_dims(self.X_train, axis=3)\n","\n","        if PREPARE_COLAB_DATA:\n","            np.save(NPY_SAVEFILE, self.X_train)\n","            #quit()\n","\n","        # Build and compile the discriminator\n","        self.discriminator = self.build_discriminator()\n","        self.discriminator.compile(loss='binary_crossentropy',\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","\n","        # Build the generator\n","        self.generator = self.build_generator()\n","\n","        # The generator takes noise as input and generates imgs\n","        z = Input(shape=(self.latent_dim,))\n","        img = self.generator(z)\n","\n","        # For the combined model we will only train the generator\n","        self.discriminator.trainable = False\n","\n","        # The discriminator takes generated images as input and determines validity\n","        validity = self.discriminator(img)\n","\n","        # The combined model  (stacked generator and discriminator)\n","        # Trains the generator to fool the discriminator\n","        self.combined = Model(z, validity)\n","        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n","\n","\n","    def build_generator(self):\n","\n","        model = Sequential()\n","\n","        model.add(Dense(256, input_dim=self.latent_dim))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization(momentum=0.8))\n","\n","        model.add(Dense(512))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization(momentum=0.8))\n","\n","        model.add(Dense(1024))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization(momentum=0.8))\n","\n","        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n","        model.add(Reshape(self.img_shape))\n","\n","        model.summary()\n","\n","        noise = Input(shape=(self.latent_dim,))\n","        img = model(noise)\n","\n","        return Model(noise, img)\n","\n","    def build_discriminator(self):\n","\n","        model = Sequential()\n","\n","\n","        model.add(Conv2D(16, (5, 5),\n","                         activation='relu',\n","                         padding='same',\n","                         input_shape=self.img_shape))\n","\n","        model.add(Conv2D(8, (7, 7),\n","                         activation='relu',\n","                         padding='same',\n","                         input_shape=self.img_shape))\n","\n","        model.add(Flatten(input_shape=self.img_shape))\n","        model.add(Dense(512))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dense(1, activation='sigmoid'))\n","        model.summary()\n","\n","        img = Input(shape=self.img_shape)\n","        validity = model(img)\n","\n","        return Model(img, validity)\n","\n","    def train(self, epochs, batch_size=1, sample_interval=50):\n","\n","        # Adversarial ground truths\n","        valid = np.ones((batch_size, 1))\n","        fake = np.zeros((batch_size, 1))\n","\n","        tensorboard = TensorBoard(log_dir=self.logdir)\n","        tensorboard.set_model(self.discriminator)\n","\n","        for idx in range(0, len(AUGMENTED_FILES)):\n","            for epoch in range(epochs):\n","\n","                # ---------------------\n","                #  Train Discriminator\n","                # ---------------------\n","\n","                # Select a random batch of images\n","                idx = np.random.randint(0, self.X_train.shape[0], batch_size)\n","                imgs = self.X_train[idx]\n","\n","                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n","\n","                # Generate a batch of new images\n","                gen_imgs = self.generator.predict(noise)\n","\n","                if epoch == 0 or accuracy < 80:\n","                    # Train the discriminator\n","                    d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n","                    d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n","                    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","                else:\n","                    # Test the discriminator\n","                    d_loss_real = self.discriminator.test_on_batch(imgs, valid)\n","                    d_loss_fake = self.discriminator.test_on_batch(gen_imgs, fake)\n","                    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","                accuracy = 100*d_loss[1]\n","\n","                # ---------------------\n","                #  Train Generator\n","                # ---------------------\n","\n","                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n","\n","                if epoch == 0 or accuracy > 20:\n","                    # Train the generator (to have the discriminator label samples as valid)\n","                    g_loss = self.combined.train_on_batch(noise, valid)\n","                else:\n","                    # Train the generator (to have the discriminator label samples as valid)\n","                    g_loss = self.combined.test_on_batch(noise, valid)\n","\n","                tensorboard.on_epoch_end(epoch, {'generator loss': g_loss, 'discriminator loss': d_loss[0], 'Accuracy': accuracy})\n","\n","                # Plot the progress\n","                if RUN_ON_COLAB:\n","                    if (epoch % 200) == 0:\n","                        print(f\"{epoch} [D loss: {d_loss[0]}, \" +\n","                      f\"acc.: {accuracy}%] [G loss: {g_loss}]\")\n","                else:\n","                    print(f\"{epoch} [D loss: {d_loss[0]}, \" +\n","                      f\"acc.: {accuracy}%] [G loss: {g_loss}]\")\n","\n","                # If at save interval => save generated image samples\n","                if epoch % sample_interval == 0:\n","                    self.sample_images(epoch)\n","            self.X_train = np.load(AUGMENTED_FILES[idx])\n","        tensorboard.on_train_end()\n","\n","    def sample_images(self, epoch):\n","        r, c = 5, 5\n","        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n","        gen_imgs = self.generator.predict(noise)\n","\n","        # Rescale images from [-1, 1] to [1, 0] (invert)\n","        gen_imgs = -0.5 * gen_imgs - 0.5\n","\n","        fig, axs = plt.subplots(r, c)\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n","                axs[i,j].axis('off')\n","                cnt += 1\n","        fig.savefig(IMAGE_DIR+\"%d.png\" % epoch)\n","        plt.close()\n","\n","\n","if __name__ == '__main__':\n","    gan = GAN()\n","    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","Log directory already exists!\n","Output images directory already exists!\n","(510, 75, 96, 1)\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_1 (Conv2D)            (None, 75, 96, 16)        416       \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 75, 96, 8)         6280      \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 57600)             0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 512)               29491712  \n","_________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 513       \n","=================================================================\n","Total params: 29,498,921\n","Trainable params: 29,498,921\n","Non-trainable params: 0\n","_________________________________________________________________\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_3 (Dense)              (None, 256)               25856     \n","_________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 256)               1024      \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 512)               131584    \n","_________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 512)               2048      \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 1024)              525312    \n","_________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 1024)              4096      \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 7200)              7380000   \n","_________________________________________________________________\n","reshape_1 (Reshape)          (None, 75, 96, 1)         0         \n","=================================================================\n","Total params: 8,069,920\n","Trainable params: 8,066,336\n","Non-trainable params: 3,584\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","0 [D loss: 0.9100474715232849, acc.: 34.375%] [G loss: 0.029758142307400703]\n","200 [D loss: 0.7344865202903748, acc.: 78.125%] [G loss: 7.555872917175293]\n","400 [D loss: 0.19603601098060608, acc.: 96.875%] [G loss: 6.834625720977783]\n","600 [D loss: 0.25728583335876465, acc.: 90.625%] [G loss: 5.580780029296875]\n","800 [D loss: 0.3726247549057007, acc.: 81.25%] [G loss: 4.761806964874268]\n","1000 [D loss: 0.27107375860214233, acc.: 90.625%] [G loss: 5.047158241271973]\n","1200 [D loss: 0.20135806500911713, acc.: 100.0%] [G loss: 5.906414985656738]\n","1400 [D loss: 0.5352913737297058, acc.: 62.5%] [G loss: 6.928520679473877]\n","1600 [D loss: 0.20185494422912598, acc.: 96.875%] [G loss: 6.469662189483643]\n","1800 [D loss: 0.1314624547958374, acc.: 93.75%] [G loss: 7.300889492034912]\n","2000 [D loss: 0.10830563306808472, acc.: 100.0%] [G loss: 2.5080909729003906]\n","2200 [D loss: 0.24941760301589966, acc.: 93.75%] [G loss: 6.008194923400879]\n","2400 [D loss: 0.18080344796180725, acc.: 90.625%] [G loss: 5.822521209716797]\n","2600 [D loss: 0.17348171770572662, acc.: 90.625%] [G loss: 4.297013759613037]\n","2800 [D loss: 0.18628458678722382, acc.: 96.875%] [G loss: 4.3980865478515625]\n","3000 [D loss: 0.32766950130462646, acc.: 90.625%] [G loss: 2.7292709350585938]\n","3200 [D loss: 0.17802171409130096, acc.: 93.75%] [G loss: 3.5895700454711914]\n","3400 [D loss: 0.36299359798431396, acc.: 84.375%] [G loss: 1.313931941986084]\n","3600 [D loss: 0.25921201705932617, acc.: 93.75%] [G loss: 2.4220361709594727]\n","3800 [D loss: 0.24613192677497864, acc.: 93.75%] [G loss: 3.0153188705444336]\n","4000 [D loss: 0.23916904628276825, acc.: 93.75%] [G loss: 1.3687078952789307]\n","4200 [D loss: 0.3452984094619751, acc.: 87.5%] [G loss: 1.8378393650054932]\n","4400 [D loss: 0.21296986937522888, acc.: 93.75%] [G loss: 2.0063669681549072]\n","4600 [D loss: 0.48416072130203247, acc.: 65.625%] [G loss: 1.1036866903305054]\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-690bdfd3a7e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSAMPLE_INTERVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-690bdfd3a7e3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                     \u001b[0;31m# Train the generator (to have the discriminator label samples as valid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m                     \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                     \u001b[0;31m# Train the generator (to have the discriminator label samples as valid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}